#!/usr/bin/env python

import sys, os, argparse, subprocess, shutil
from pispino.runcmd import *
from pispino.logger import *

__version__    = 2.8

__author__     = "Hyun Soon Gweon"
__copyright__  = "Copyright 2015, The PIPITS Project"
__credits__    = ["Hyun Soon Gweon", "Anna Oliver", "Joanne Taylor", "Tim Booth", "Melanie Gibbs", "Daniel S. Read", "Robert I. Griffiths", "Karsten Schonrogge"]
__license__    = "GPL"
__maintainer__ = "Hyun Soon Gweon"
__email__      = "h.s.gweon@reading.ac.uk"

VSEARCH = "vsearch"
BIOM = "biom"

if __name__ == '__main__':

    parser = argparse.ArgumentParser("PIPITS_PROCESS: Sequences to OTU Table")
    parser.add_argument(
        "-i",
        action = "store",
        dest = "input",
        metavar = "<FILE>",
        help = "[REQUIRED] ITS sequences in FASTA. Typically output from pipits_funits",
        required = True)
    parser.add_argument(
        "-o",
        action = "store",
        dest = "outDir",
        metavar = "<DIR>",
        default = "pipits_process",
        help = "[REQUIRED] Directory to output results.",
        required = False)
    parser.add_argument(
        "-d",
        action = "store",
        dest = "VSEARCH_id",
        metavar = "<FLOAT>",
        help = "VSEARCH - Identity threshold [default: 0.97]",
        default = "0.97",
        required = False)
    parser.add_argument(
        "-c",
        action = "store",
        dest = "RDP_assignment_threshold",
        metavar = "<FLOAT>",
        help = "RDP assignment confidence threshold - RDP Classifier confidence threshold for output [default: 0.85]",
        default = "0.85",
        required = False)
    parser.add_argument(
        "-l",
        action = "store",
        dest = "sortedlist",
        metavar = "<TXT>",
        help = "[REQUIRED] Sample list file. Generated with PIPITS_GETREADPAIRSLIST prior to PIPITS_PREP",
        required = False)
    parser.add_argument(
        "--includeuniqueseqs",
        action = "store_true",
        dest = "includeuniqueseqs",
        help = "[REQUIRED] PIPITS by default removes unique sequences before clustering. This means you wouldn't have any singletons. If you want singletons, then choose this option. It can take much longer to process.",
        required = False)
    # Generic
    parser.add_argument(
        "-r",
        action = "store_true",
        dest = "retain",
        help = "Retain intermediate files (Beware intermediate files use excessive disk space!)",
        required = False)
    parser.add_argument(
        "-v",
        action = "store_true",
        dest = "verbose",
        help = "Verbose mode",
        required = False)
    parser.add_argument(
        "-t",
        action = "store",
        dest = "threads",
        metavar = "<INT>",
        help = "Number of Threads [default: 1]",
        default = "1",
        required = False)
    parser.add_argument(
        "--Xms",
        action = "store",
        dest = "Xms",
        default = "4g",
        metavar = "<INT>",
        help = "The minimum size, in bytes, of the memory allocation pool for JVM",
        required = False)
    parser.add_argument(
        "--Xmx",
        action = "store",
        dest = "Xmx",
        default = "4g",
        metavar = "<INT>",
        help = "The maximum size, in bytes, of the memory allocation pool for JVM",
        required = False)
    parser.add_argument(
        "--warcup",
        action = "store_true",
        dest = "warcup",
        help = "Also classify using Warcup database. The most recent warcup db will be downloaded automatically.",
        default = False,
        required = False)
    parser.add_argument(
        "--unite",
        action = "store",
        dest = "unite",
        help = "UNITE db version to be used - PIPITS will download db automaticlly. Leaving this option out will default to the most recent version of UNITE available to PIPITS. Currently 02_02_2019.",
        choices = ["10.05.2021", "04.02.2020", "02.02.2019", "01.12.2017", "28.06.2017"],
        default = "10.05.2021",
        required = False)
    options = parser.parse_args()


    ######################
    # Create directories #
    ######################

    EXE_DIR = os.path.dirname(os.path.realpath(__file__))
    if not os.path.exists(options.outDir):
        os.mkdir(options.outDir)
    else:
        shutil.rmtree(options.outDir)
        os.mkdir(options.outDir)
    tmpDir = options.outDir + "/intermediate"
    if not os.path.exists(tmpDir):
        os.mkdir(tmpDir)


    #############
    # Log files #
    #############

    logging_file = open(options.outDir + "/output.log", "w")
    summary_file = open(options.outDir + "/summary.log", "w")
    version_file = open(options.outDir + "/versions.log", "w")


    ##########
    # Start! #
    ##########

    logger("pipits_process " + str(__version__) + ", the PIPITS Project", logging_file, display = True, timestamp = False)
    logger("https://github.com/hsgweon/pipits", logging_file, display = True, timestamp = False)
    logger("---------------------------------", logging_file, display = True, timestamp = False)
    logger("", logging_file, display = True, timestamp = False)

    # Log versions
    cmd = " ".join(["conda list"])
    run_cmd(cmd, version_file, False)

    # Environment variable - discontinued.
    # 
    # EXIT = []
    # if os.environ.get("PIPITS_UNITE_REFERENCE_DATA_CHIMERA") is None:
    #     EXIT.append("PIPITS_UNITE_REFERENCE_DATA_CHIMERA")
    # if os.environ.get("PIPITS_UNITE_RETRAINED_DIR") is None:
    #     EXIT.append("PIPITS_UNITE_RETRAINED_DIR")
    # if os.environ.get("PIPITS_WARCUP_RETRAINED_DIR") is None:
    #     EXIT.append("PIPITS_WARCUP_RETRAINED_DIR")
    # 
    # if len(EXIT) > 0:
    #     logger("ERRO: environment variables (" + ", ".join(EXIT) + ") are not set. Please see PIPITS installation for an instruction on this.", logging_file, display = True)
    #     exit(0)
    # 

    # Start
    logger(ENDC + "pipits_process started" + ENDC, logging_file, display = True)

    # Check for input file
    if not os.path.exists(options.input):
        logger("ERROR: Input file doesn't exist", logging_file, display = True)
        exit(1)

    if os.stat(options.input).st_size == 0:
        logger("Input file is empty!", logging_file, display = True)
        exit(0)


    ##############
    # Sample ids #
    ##############

    if not options.sortedlist:

        # Generating a sample list
        logger(ENDC + "Generating a sample list from the input sequences" + ENDC, logging_file, display = True)

        try:
            os.remove(tmpDir + "/sampleIDs.txt")
        except OSError:
            pass

        cmd = " ".join(["python", EXE_DIR + "/pipits_getsamplelistfromfasta",
                        "-i", options.input,
                        "-o", options.outDir + "/sampleIDs.txt"])
        run_cmd(cmd, logging_file, options.verbose)
        options.sortedlist = options.outDir + "/sampleIDs.txt"


    sampleIDs = []
    if not os.path.exists(options.sortedlist):
        print("ERROR: Sample list file does NOT exist")
        exit(1)

    for line in open(options.sortedlist):
        if line[0] != "#":
            sampleIDs.append(line.split("\t")[0].rstrip())

    # Check for duplicate entries
    duplicateIDs = list(set([x for x in sampleIDs if sampleIDs.count(x) > 1]))
    if len(duplicateIDs) > 0:
        print("ERROR: You have a duplicate id in your Sample list file. Offending id: " + ",".join(duplicateIDs))
        exit(1)


    #################
    # Download data #
    #################

    def get_md5(filename):
    
        import hashlib

        hash_md5 = hashlib.md5()
        with open(filename, "rb") as f:
            for chunk in iter(lambda: f.read(4096), b""):
                hash_md5.update(chunk)
        return hash_md5.hexdigest()

    def downloadDB(
        url,
        md5,
        output_dir,
        logging_file, 
        summary_file, 
        verbose):

        import progressbar
        import requests
        import tarfile
        from pathlib import Path

        if not os.path.exists(output_dir):
            os.mkdir(output_dir)
        # else:
        #     shutil.rmtree(output_dir)
        #     os.mkdir(output_dir)

        filename     = url.split("/")[-1]

        # Check if unpacked file exists
        DOWNLOADDB = True
        
        if os.path.isfile(output_dir + "/" + filename):

            if get_md5(output_dir + "/" + filename) != md5:
                logger("... DB directory and files exits, but seems to be different/corrupt -> re-downloading...", logging_file, display = True)
                DOWNLOADDB = True
            else:
                logger("... DB directory and files exits, and all looking good. No need to download.", logging_file, display = True)
                DOWNLOADDB = False

        if DOWNLOADDB:

            request = requests.get(url, stream=True)

            file         = open(output_dir + "/" + filename, 'wb')
            file_size    = int(request.headers['Content-Length'])
            file_size_mb = round(file_size / 1024 / 1024,2)

            chunk_sz = 512

            widgets = [progressbar.Bar(marker="#",left="[",right="]"),
                       progressbar.Percentage()," | ",
                       progressbar.FileTransferSpeed()," | ",
                       progressbar.SimpleProgress()," | ",
                       progressbar.ETA()]

            bar = progressbar.ProgressBar(widgets=widgets, maxval=file_size).start()

            i = 0

            for chunk in request.iter_content(chunk_size=chunk_sz):
                file.write(chunk)
                i += len(chunk)
                bar.update(i)

            bar.finish()
            print('File size: {0} MB'.format(file_size_mb))

            file.close()

            if get_md5(output_dir + "/" + filename) != md5:
                print("Downloaded data is corrupt. Get in touch with PIPITS team!. Exiting...")
                exit(1)

        # Unpack
        logger("... Unpacking", logging_file, display = True)

        dirname = output_dir + "/" + filename.split(".tar.gz")[0]
        if not os.path.exists(dirname):
            os.mkdir(dirname)

        tar = tarfile.open(output_dir + "/" + filename)
        tar.extractall(path=dirname)
        tar.close()


    #########
    # UNITE #
    #########

    logger("Downloading UNITE trained database, version: " + options.unite, logging_file, display = True)

    if options.unite == "02.02.2019":
        url = "https://sourceforge.net/projects/pipits/files/PIPITS_DB/UNITE_retrained_" + options.unite + ".tar.gz"
        md5 = "8fd3b74a510bb20b67933a2ecc620f89"
    elif options.unite == "01.12.2017":
        url = "https://sourceforge.net/projects/pipits/files/PIPITS_DB/UNITE_retrained_" + options.unite + ".tar.gz"
        md5 = "3c5be9c60fecf70076739379e7c9ead5"
    elif options.unite == "28.06.2017":
        url = "https://sourceforge.net/projects/pipits/files/PIPITS_DB/UNITE_retrained_" + options.unite + ".tar.gz"
        md5 = "33fa78987751a494c586676ff3a0da65"
    elif options.unite == "04.02.2020":
        url = "https://sourceforge.net/projects/pipits/files/PIPITS_DB/UNITE_retrained_" + options.unite + ".tar.gz"
        md5 = "b2f833c89794be20a5fdb6169d9205f1"
    elif options.unite == "10.05.2021":
        url = "https://sourceforge.net/projects/pipits/files/PIPITS_DB/UNITE_retrained_" + options.unite + ".tar.gz"
        md5 = "13f1edfb1357eeda3f41ff8b0a15447f"
        
    downloadDB(
        url = url,
        md5 = md5,
        output_dir = "pipits_db",
        logging_file = logging_file,
        summary_file = summary_file,
        verbose = options.verbose)
    logger(BLUE + "... done" + ENDC, logging_file, display = True)


    ##########
    # WARCUP #
    ##########

    logger("Downloading WARCUP trained database: ", logging_file, display = True)

    url = "https://sourceforge.net/projects/pipits/files/PIPITS_DB/warcup_retrained_V2.tar.gz"
    md5 = "e84733f23121f00de03b2a9b5398d6fb"

    downloadDB(
        url = url,
        md5 = md5,
        output_dir = "pipits_db",
        logging_file = logging_file,
        summary_file = summary_file,
        verbose = options.verbose)
    logger(BLUE + "... done" + ENDC, logging_file, display = True)


    ##########
    # UCHIME #
    ##########

    logger("Downloading UCHIME database for chimera filtering: ", logging_file, display = True)

    url = "https://sourceforge.net/projects/pipits/files/PIPITS_DB/uchime_reference_dataset_28.06.2017.tar.gz"
    md5 = "a84af781f9ba42a76f456b558dbc1ae5"

    downloadDB(
        url = url,
        md5 = md5,
        output_dir = "pipits_db",
        logging_file = logging_file,
        summary_file = summary_file,
        verbose = options.verbose)
    logger(BLUE + "... done" + ENDC, logging_file, display = True)



    #########        
    # Derep #
    #########        

    minuniquesize = 2 # This removes singletons
    if options.includeuniqueseqs:
        minuniquesize = 1 # Option to include singletons

    logger(ENDC + "Dereplicating and removing unique sequences prior to picking OTUs" + ENDC, logging_file, display = True)
    cmd = " ".join([
        VSEARCH, 
        "--derep_fulllength", options.input, 
        "--output", tmpDir + "/input_nr.fasta", 
        "--minuniquesize", str(minuniquesize), 
        "--sizeout"
        ])
    run_cmd(cmd, logging_file, options.verbose)

    # Check if the file is empty
    if os.stat(tmpDir + "/input_nr.fasta").st_size == 0:
        logger(RED + "ERROR: After dereplicating and removing unique sequences, there are no sequences! Processing stopped." + ENDC, logging_file, display = True)
        exit(0)


    ##################
    # OTU clustering #
    ##################

    logger(ENDC + "Picking OTUs [VSEARCH]" + ENDC, logging_file, display = True)
    cmd = " ".join([VSEARCH, 
                    "--cluster_fast", tmpDir + "/input_nr.fasta", 
                    "--id", options.VSEARCH_id,
                    "--centroids", tmpDir + "/input_nr_otus.fasta",
                    "--uc", tmpDir + "/input_nr_otus.uc",
                    "--threads", options.threads])
    run_cmd(cmd, logging_file, options.verbose)


    ###################
    # Chimera removal #
    ###################

    logger(ENDC + "Removing chimeras [VSEARCH]" + ENDC, logging_file, display = True)
    PIPITS_UNITE_REFERENCE_DATA_CHIMERA = "pipits_db/uchime_reference_dataset_28.06.2017/uchime_reference_dataset_28.06.2017.fasta"
    cmd = " ".join([
        VSEARCH, 
        "--uchime_ref", tmpDir + "/input_nr_otus.fasta", 
        "--db " + PIPITS_UNITE_REFERENCE_DATA_CHIMERA,
        "--nonchimeras", tmpDir + "/input_nr_otus_nonchimeras.fasta",
        "--threads", options.threads])
    run_cmd(cmd, logging_file, options.verbose)


    ###############
    # Rename OTUs #
    ###############

    logger(ENDC + "Renaming OTUs" + ENDC, logging_file, display = True)
    def renumberOTUS():
        handle_in = open(tmpDir + "/input_nr_otus_nonchimeras.fasta", "rU")
        handle_out = open(tmpDir + "/input_nr_otus_nonchimeras_relabelled.fasta", "w")

        counter = 1
        
        for line in handle_in:
            if line.startswith(">"):
                newlabel = line[1:].split(";")[0]
                handle_out.write(">OTU" + str(counter) + "\n")
                counter += 1
            else:
                handle_out.write(line.rstrip() + "\n")
        handle_in.close()
        handle_out.close()
    renumberOTUS()


    #####################
    # Map reads to OTUs #
    #####################

    logger(ENDC + "Mapping reads onto centroids [VSEARCH]" + ENDC, logging_file, display = True)
    cmd = " ".join([VSEARCH, 
                    "--usearch_global", options.input, 
                    "--db", tmpDir + "/input_nr_otus_nonchimeras_relabelled.fasta", 
                    "--id", options.VSEARCH_id, 
                    "--uc", tmpDir + "/otus.uc",
                    "--threads", options.threads])
    run_cmd(cmd, logging_file, options.verbose)


    ####################
    # OTU construction #
    ####################

    logger(ENDC + "Making OTU table" + ENDC, logging_file, display = True)
    cmd = " ".join(["python", EXE_DIR + "/pipits_uc2otutable", 
                    "-i", tmpDir + "/otus.uc", 
                    "-o", tmpDir + "/otu_table_prelim.txt",
                    "-l", options.sortedlist])
    run_cmd(cmd, logging_file, options.verbose)


    ###################
    # Convert to biom #
    ###################

    logger(ENDC + "Converting classic tabular OTU into a BIOM format [BIOM]" + ENDC, logging_file, display = True)
    try:
        os.remove(tmpDir + "/otu_table_prelim.biom")
    except OSError:
        pass
    cmd = " ".join([
        BIOM, "convert", 
        "-i", tmpDir + "/otu_table_prelim.txt", 
        "-o", tmpDir + "/otu_table_prelim.biom", 
        "--table-type=\"OTU table\" --to-hdf5"
        ])
    run_cmd(cmd, logging_file, options.verbose)


    #####################
    # RDP against UNITE #
    #####################

    logger(ENDC + "Assigning taxonomy with UNITE [RDP Classifier]" + ENDC, logging_file, display = True)
    PIPITS_UNITE_RETRAINED_PROPERTIES = "pipits_db/UNITE_retrained_" + options.unite + "/UNITE_retrained/rRNAClassifier.properties"
    cmd = " ".join(["classifier",
                    "-Xms" + str(options.Xms),
                    "-Xmx" + str(options.Xmx),
                    "classify", 
                    "-t " + PIPITS_UNITE_RETRAINED_PROPERTIES, 
                    "-o", options.outDir + "/assigned_taxonomy.txt", 
                    tmpDir + "/input_nr_otus_nonchimeras_relabelled.fasta"])
    run_cmd(cmd, logging_file, options.verbose)


    ###############################################
    # Reformatting RDP_CLASSIFIER output for biom #
    ###############################################

    logger(ENDC + "Reformatting RDP_Classifier output" + ENDC, logging_file, display = True)
    cmd = " ".join([
        "python", EXE_DIR + "/pipits_reformatAssignedTaxonomy", 
        "-i", options.outDir + "/assigned_taxonomy.txt" , 
        "-o", options.outDir + "/assigned_taxonomy_reformatted_filtered.txt",
        "-c", options.RDP_assignment_threshold])
    run_cmd(cmd, logging_file, options.verbose)


    #############################################
    # Adding RDP_CLASSIFIER output to OTU table #
    #############################################

    logger(ENDC + "Adding assignment to OTU table [BIOM]" + ENDC, logging_file, display = True)
    try:
        os.remove(options.outDir + "/otu_table.biom")
    except OSError:
        pass
    cmd = " ".join([
        BIOM, "add-metadata", 
        "-i", tmpDir + "/otu_table_prelim.biom", 
        "-o", options.outDir + "/otu_table.biom", 
        "--observation-metadata-fp", options.outDir + "/assigned_taxonomy_reformatted_filtered.txt", 
        "--observation-header", "OTUID,taxonomy,confidence", 
        "--sc-separated", "taxonomy", 
        "--float-fields", "confidence"])
    run_cmd(cmd, logging_file, options.verbose)


    # Convert BIOM to TABLE
    logger(ENDC + "Converting OTU table with taxa assignment into a BIOM format [BIOM]" + ENDC, logging_file, display = True)
    try:
        os.remove(options.outDir + "/otu_table.txt")
    except OSError:
        pass
    cmd = " ".join([BIOM, "convert", 
                    "-i", options.outDir + "/otu_table.biom", 
                    "-o", options.outDir + "/otu_table.txt",
                    "--to-tsv",
                    "--header-key taxonomy"])
    run_cmd(cmd, logging_file, options.verbose)

    # Phylotype
    logger(ENDC + "Phylotyping OTU table" + ENDC, logging_file, display = True)
    cmd = " ".join([
        "python", EXE_DIR + "/pipits_phylotype_biom", 
        "-i", options.outDir + "/otu_table.biom", 
        "-o", options.outDir + "/phylotype_table.txt",
        "-l 6"])
    run_cmd(cmd, logger, options.verbose)

    try:
        os.remove(options.outDir + "/phylotype_table.biom")
    except OSError:
        pass
    cmd = " ".join([
        BIOM, "convert",
        "-i", options.outDir + "/phylotype_table.txt",
        "-o", options.outDir + "/phylotype_table.biom",
        "--table-type=\"OTU table\"",
        "--to-hdf5"])
    run_cmd(cmd, logging_file, options.verbose)


    ##########
    # Warcup #
    ##########

    if options.warcup:

        # Classify
        logger("Assigning taxonomy [RDP Classifier] - Warcup", logging_file, display = True)
        PIPITS_WARCUP_RETRAINED_PROPERTIES = "pipits_db/warcup_retrained_V2/warcup_retrained_V2/rRNAClassifier.properties"
        cmd = " ".join([
            "classifier",
            "-Xms" + str(options.Xms),
            "-Xmx" + str(options.Xmx),
            "classify", 
            "-t " + PIPITS_WARCUP_RETRAINED_PROPERTIES,
            "-o", options.outDir + "/assigned_taxonomy_warcup.txt", 
            tmpDir + "/input_nr_otus_nonchimeras_relabelled.fasta"])
        run_cmd(cmd, logging_file, options.verbose)

        # Reformatting RDP_CLASSIFIER output for biom for Warcup
        cmd = " ".join([
            "python", EXE_DIR + "/pipits_reformatAssignedTaxonomy",
            "-i", options.outDir + "/assigned_taxonomy_warcup.txt" ,
            "-o", options.outDir + "/assigned_taxonomy_reformatted_filtered_warcup.txt",
            "-c", options.RDP_assignment_threshold])
        run_cmd(cmd, logging_file, options.verbose)


        # Adding RDP_CLASSIFIER output to OTU table
        logger("Adding assignment to OTU table [BIOM] - Warcup", logging_file, display = True)
        try:
            os.remove(options.outDir + "/otu_table_warcup.biom")
        except OSError:
            pass
        cmd = " ".join([
            BIOM, "add-metadata",
            "-i", tmpDir + "/otu_table_prelim.biom",
            "-o", options.outDir + "/otu_table_warcup.biom",
            "--observation-metadata-fp", options.outDir + "/assigned_taxonomy_reformatted_filtered_warcup.txt",
            "--observation-header", "OTUID,taxonomy,confidence",
            "--sc-separated", "taxonomy",
            "--float-fields", "confidence"])
        run_cmd(cmd, logging_file, options.verbose)


        # Convert BIOM to TABLE
        logger("Converting OTU table with taxa assignment into a BIOM format [BIOM] - Warcup", logging_file, display = True)
        try:
            os.remove(options.outDir + "/otu_table_warcup.txt")
        except OSError:
            pass
        cmd = " ".join([
            BIOM, "convert",
            "-i", options.outDir + "/otu_table_warcup.biom",
            "-o", options.outDir + "/otu_table_warcup.txt",
            "--to-tsv",
            "--header-key taxonomy"])
        run_cmd(cmd, logging_file, options.verbose)


        # Make phylotyp table #
        logger("Phylotyping OTU table - Warcup", logging_file, display = True)
        cmd = " ".join([
            "python", EXE_DIR + "/pipits_phylotype_biom", 
            "-i", options.outDir + "/otu_table_warcup.biom", 
            "-o", options.outDir + "/phylotype_table_warcup.txt",
            "-l 8"])
        run_cmd(cmd, logging_file, options.verbose)

        try:
            os.remove(options.outDir + "/phylotype_table_warcup.biom")
        except OSError:
            pass
        cmd = " ".join([
            BIOM, "convert",
            "-i", options.outDir + "/phylotype_table_warcup.txt",
            "-o", options.outDir + "/phylotype_table_warcup.biom",
            "--table-type=\"OTU table\"",
            "--to-hdf5"])
        run_cmd(cmd, logging_file, options.verbose)


    # Move representative sequence file to outDir
    shutil.move(tmpDir + "/input_nr_otus_nonchimeras_relabelled.fasta", options.outDir + "/repseqs.fasta")


    # Remove tmp
    if not options.retain:
        logger(ENDC + "Cleaning temporary directory" + ENDC, logging_file, display = True)
        shutil.rmtree(tmpDir)


    ####################################
    # Stats: Import json formatted OTU #
    ####################################

    def biomstats(BIOMFILE):

        from biom import load_table
        biom = load_table(BIOMFILE)

        sampleSize = int(biom.shape[1])
        otus = int(biom.shape[0])
        totalCount = biom.sum()

        return totalCount, otus, sampleSize

    otu_reads_count, otu_count, otu_sample_count = biomstats(options.outDir + "/otu_table.biom")
    phylo_reads_count, phylo_count, phylo_sample_count = biomstats(options.outDir + "/phylotype_table.biom")

    summary_file.write("No.of reads used to generate OTU table: " + str(int(otu_reads_count)) + "\n")
    summary_file.write("Number of OTUs:                         " + str(otu_count) + "\n")
    summary_file.write("Number of phylotypes:                   " + str(phylo_count) + "\n")
    summary_file.write("Number of samples:                      " + str(otu_sample_count) + "\n\n")
    summary_file.write("UNITE DB version:                       " + options.unite + "\n")
    summary_file.write("PIPITS version:                         " + str(__version__) + "\n")

    logger(BLUE + "\tNumber of reads used to generate OTU table: " + str(int(otu_reads_count)) + ENDC, logging_file, display = True)
    logger(BLUE + "\tNumber of OTUs:                             " + str(otu_count) + ENDC, logging_file, display = True)
    logger(BLUE + "\tNumber of phylotypes:                       " + str(phylo_count) + ENDC, logging_file, display = True)
    logger(BLUE + "\tNumber of samples:                          " + str(otu_sample_count) + ENDC, logging_file, display = True)

    logger(ENDC + "Done - Resulting files are in \"" + options.outDir + "\" directory" + ENDC, logging_file, display = True)
    logger(ENDC + "pipits_process ended successfully." + ENDC, logging_file, display = True)

    logging_file.close()
    version_file.close()
    summary_file.close()
